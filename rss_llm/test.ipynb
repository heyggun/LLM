{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. rss (키워드 검색)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import feedparser\n",
    "import ssl\n",
    "import urllib.request\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corps = set()\n",
    "\n",
    "keyword = '오늘날씨'\n",
    "\n",
    "rss_url = f\"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko\"\n",
    "# rss_url = f\"https://news.google.com/rss/search?q={keyword}\"\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "results = []\n",
    "for entry in feed.entries:\n",
    "    result = {\n",
    "        'title': entry.title,\n",
    "        'link': entry.link,\n",
    "        'published': entry.published,\n",
    "        'corp' : entry.title.split(' - ')[1]\n",
    "    }\n",
    "    results.append(result)\n",
    "    corps.add(entry.title.split(' - ')[1])\n",
    "\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import feedparser\n",
    "from newspaper import Article\n",
    "import datetime\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "class NewsCrawler():\n",
    "    def __init__(self):\n",
    "        self.user_agent = UserAgent()\n",
    "        self.headers = None\n",
    "\n",
    "    def extract_tag(self, item, tag):\n",
    "        [elem.extract() for elem in item(tag)]\n",
    "\n",
    "\n",
    "    def extract_comments(self, item):\n",
    "        for element in item(text=lambda text: isinstance(text, Comment)):\n",
    "            element.extract()\n",
    "            \n",
    "    def get_headers(self, shuffle_header=False):\n",
    "        if self.headers is None or shuffle_header:\n",
    "            self.headers = {\n",
    "                'User-Agent': self.user_agent.random,\n",
    "            }\n",
    "        return self.headers\n",
    "\n",
    "\n",
    "    def crawl_url(self, url, shuffle_header=False, language='ko'):\n",
    "        try:\n",
    "            # article title\n",
    "            article = Article(url, language=language)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            if not article.url:\n",
    "                print(\"[ERROR] no url found..\")\n",
    "                return None        \n",
    "\n",
    "            data = {}\n",
    "            data['title'] = article.title\n",
    "            data['image'] = article.meta_img\n",
    "            data['crawl_url'] = url\n",
    "            data['url'] = article.url\n",
    "\n",
    "            print(article.url)\n",
    "            if article.publish_date:\n",
    "                data['published_at'] = article.publish_date\n",
    "            else:\n",
    "                data['published_at'] = datetime.datetime.now()\n",
    "\n",
    "            res = requests.get(url, headers=self.get_headers(shuffle_header=shuffle_header))\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            for script in soup(['script', 'style']):\n",
    "                script.decompose()\n",
    "\n",
    "            body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "\n",
    "            if body is None:\n",
    "                body = soup.select('#articleBodyContents')\n",
    "                if len(body) > 0:\n",
    "                    body = body[0]\n",
    "                    self.extract_comments(body)\n",
    "                    self.extract_tag(body, 'img')\n",
    "                    self.extract_tag(body, 'div')\n",
    "                    body = str(body)\n",
    "                else:\n",
    "                    body = article.text\n",
    "                    body = body.replace('\\n','<br>')\n",
    "            else:\n",
    "                self.extract_comments(body)\n",
    "                self.extract_tag(body, 'img')\n",
    "                self.extract_tag(body, 'div')\n",
    "                body = str(body)\n",
    "\n",
    "            data['text'] = article.text\n",
    "            data['body'] = body\n",
    "            data['language'] = language\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            data = None\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def crawl_rss(self, rss_url, shuffle_header=False, language='ko'):\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        data_list = []\n",
    "        for post in feed.entries:\n",
    "            if post.link:\n",
    "                data_list.append(self.crawl_url(post.link, shuffle_header, language))\n",
    "\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_url = \"\"\"https://news.google.com/rss/articles/CBMiMGh0dHBzOi8vd3d3Lm5ld3NwaW0uY29tL25ld3Mvdmlldy8yMDI0MDUwODAwMDAxOdIBMWh0dHBzOi8vbS5uZXdzcGltLmNvbS9uZXdzYW1wL3ZpZXcvMjAyNDA1MDgwMDAwMTk?oc=5\n",
    "\"\"\"\n",
    "\n",
    "c = NewsCrawler()\n",
    "# tmp = c.crawl_url(sample_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.DataFrame(columns=['title','image','crawl_url','url','published_at','text','body'])\n",
    "url_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = NewsCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in results:\n",
    "#     tmp = c.crawl_url(r['link'])\n",
    "#     length = len(url_df)+1\n",
    "    \n",
    "#     url_df.loc[length, 'title'] = tmp['title']\n",
    "#     url_df.loc[length, 'image'] = tmp['image']\n",
    "#     url_df.loc[length, 'crawl_url'] = tmp['crawl_url']\n",
    "#     url_df.loc[length, 'url'] = tmp['url']\n",
    "#     url_df.loc[length, 'published_at'] = tmp['published_at']\n",
    "#     url_df.loc[length, 'text'] = tmp['text']\n",
    "#     url_df.loc[length, 'body'] = tmp['body']\n",
    "    \n",
    "# url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_words = ['기자', '일보', '앵커']   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in url_df.loc[:1].iterrows():\n",
    "#     url = row['url']\n",
    "#     print(row['text'])\n",
    "#     text = row['text'].replace('\\n', '')\n",
    "#     pattern1 = r'\\b(?:' + '|'.join(list(corps)) + r')\\b'\n",
    "#     preprocessed_text = re.sub(pattern1, '', text)\n",
    "#     pattern2 = r'[^\\w\\s,~.\\'\"]'\n",
    "#     preprocessed_text = re.sub(pattern2, '', preprocessed_text)\n",
    "#     pattern3 = r'\\b(?:\\w+ 앵커|\\w+ 기자|\\w+일보)\\b'\n",
    "#     preprocessed_text = re.sub(pattern3, '', preprocessed_text)\n",
    "\n",
    "#     print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mecab import MeCab as mecab\n",
    "mecab = mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mecab-co-dic 품사 태그\n",
    "\n",
    "    - NNG : 일반명사\n",
    "    - NNP : 고유명사\n",
    "    - NNB : 의존명사\n",
    "    - NNBC : 단위를 나타내는 명사\n",
    "    - NR : 수사\n",
    "    - NP : 대명사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tagging(query):\n",
    "    print(f\" 원 쿼리 -> {query}\")\n",
    "    mecab_result = mecab.pos(query)\n",
    "    print(f\" mecab pos tagging -> {mecab_result}\")\n",
    "    result = []\n",
    "    combined_word = ''\n",
    "\n",
    "    cur = 0\n",
    "    prev_pos = None\n",
    "    while cur < len(mecab_result):\n",
    "        word, pos = mecab_result[cur]\n",
    "        if pos == 'SN':\n",
    "            combined_word += word\n",
    "            nxt = cur + 1\n",
    "            while nxt < len(mecab_result) and mecab_result[nxt][1] == 'SN':\n",
    "                combined_word += mecab_result[nxt][0]\n",
    "                nxt += 1\n",
    "                \n",
    "            while nxt < len(mecab_result) and mecab_result[nxt][1] in ['NNG', 'NNP', 'NNB', 'NNBC', 'NR', 'NP']:\n",
    "                if mecab_result[nxt-1][1] in ['NNG', 'NNP', 'NNB', 'NNBC', 'NR', 'NP']:\n",
    "                    break\n",
    "                combined_word += mecab_result[nxt][0]\n",
    "                nxt += 1\n",
    "            result.append(combined_word)\n",
    "            combined_word = ''\n",
    "            cur = nxt\n",
    "        elif pos in ['MAG', 'NNG', 'NNP', 'NNB', 'NNBC', 'NR', 'NP']:\n",
    "            combined_word = word\n",
    "            result.append(combined_word)\n",
    "            nxt = cur + 1\n",
    "            combined_word = ''\n",
    "            cur = nxt\n",
    "        else:\n",
    "            cur += 1\n",
    "\n",
    "    print(f\" 키워드 추출 -> {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_pos_tagging('내일 서울 날씨 알려줘')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검색어 가져와서 rss 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_combination_1= '+'.join(random.sample(result, len(result)))\n",
    "print(keyword_combination_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_combination_2= '+'.join(random.sample(result, len(result)))\n",
    "print(keyword_combination_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_combination_3= '+'.join(random.sample(result, len(result)))\n",
    "print(keyword_combination_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_rss_keyword(keyword_combination):\n",
    "    corps = set()\n",
    "    rss_url = f\"https://news.google.com/rss/search?q={keyword_combination}&hl=ko&gl=KR&ceid=KR:ko\"\n",
    "    # rss_url = f\"https://news.google.com/rss/search?q={keyword}\"\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    results = []\n",
    "    for entry in feed.entries:\n",
    "        result = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "            'published': entry.published,\n",
    "            'corp' : entry.title.split(' - ')[1]\n",
    "        }\n",
    "        results.append(result)\n",
    "        corps.add(entry.title.split(' - ')[1])\n",
    "        \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1 = search_rss_keyword(keyword_combination_1)\n",
    "case2 = search_rss_keyword(keyword_combination_2)\n",
    "case3 = search_rss_keyword(keyword_combination_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_call_df_1= pd.DataFrame(case1)\n",
    "url_call_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_call_df_2= pd.DataFrame(case2)\n",
    "url_call_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_call_df_3= pd.DataFrame(case3)\n",
    "url_call_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_url_df = pd.merge(pd.merge(url_call_df_1, url_call_df_2), url_call_df_3)\n",
    "final_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML 기반 유사도 높은 문서 5개 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = final_url_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['embedding'] = train_data.apply(lambda row: model.encode(row.title), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"내일 서울 날씨 알려줘\"\n",
    "\n",
    "embedding = model.encode(query)\n",
    "train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)\n",
    "sim_result_df = train_data.sort_values(by='score', ascending=False)[:5].reset_index(drop=True)\n",
    "sim_result_df.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in sim_result_df[:10].iterrows():\n",
    "    tmp = c.crawl_url(row['link'])\n",
    "    length = len(content_df)+1\n",
    "    \n",
    "    try :\n",
    "        content_df.loc[length, 'title'] = tmp['title']\n",
    "        content_df.loc[length, 'url'] = tmp['url']\n",
    "        content_df.loc[length, 'published_at'] = tmp['published_at']\n",
    "        content_df.loc[length, 'text'] = tmp['text'].replace('\\n', '')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "content_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM (Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_json= content_df.to_json(orient='records', force_ascii=False, date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = json.loads(result_data_json)\n",
    "pprint(result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM (요약) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model='gpt-3.5-turbo',\n",
    "                   temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryObject(BaseModel):\n",
    "    title: str = Field(description=\"본문의 title\")\n",
    "    url: str = Field(description=\"본문에 해당하는 url\")\n",
    "    summary: str = Field(description=\"본문을 요약한 텍스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=SummaryObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert summarizer.\"\n",
    "            \"Summarizes text content related to a given query.\"\n",
    "            ),\n",
    "            (\n",
    "            \"user\",\n",
    "            \"Please summarize the sentences according to the following requests.\"\n",
    "            \"\\nRequest:\\n\"\n",
    "            \"Please summarize the data in 1 to 3 lines as requested below.\"\n",
    "            \"\\nRequest:\\n\"\n",
    "            \"For weather-related inquiries, please summarize the published_at closest to {today}.\"\n",
    "            \"Please indicate the URL of the referenced content as the source.\"\n",
    "            \"Only content that can answer the provided query should be summarized in Korean.\"\n",
    "            \"Be sure to include the URL related to the referenced content as the source.\"\n",
    "            \"\\n\\nQuery: {query}\\n\\nContext: {context}\\n\\nSummary:\",\n",
    "                    ),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "response= chain.invoke({\"query\": query, \"context\" : result_json, \"today\" : datetime.date.today()})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
