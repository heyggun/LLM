{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolCall\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def build_pre_model_hook(\n",
    "    original_hook: Callable, *, force_tool_name: str | None, query: str\n",
    ") -> Callable:\n",
    "    def wrapped(state):\n",
    "        if force_tool_name:\n",
    "            # 강제 tool call 메시지 생성\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        tool_calls=[ToolCall(name=force_tool_name, args={\"query\": query})],\n",
    "                        content=None,\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        return original_hook(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Union\n",
    "from enum import Enum, auto, StrEnum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class OpenAIChatModelNamesEnum(StrEnum):\n",
    "    GPT4O = auto()\n",
    "    GPT4O_MINI = auto()\n",
    "    GPT4O_MINI_FREE = (\n",
    "        auto()\n",
    "    )  # GPT4O_MINI랑 동일하게 gpt-4o-mini를 사용하는데, 크레딧 과금이 안 되는 무료 모델\n",
    "    GPT_4_1 = auto()\n",
    "    GPT_4_1_MINI = auto()\n",
    "    GPT_4_1_NANO = auto()\n",
    "\n",
    "\n",
    "ChatModelNamesEnum = Union[\n",
    "    OpenAIChatModelNamesEnum,\n",
    "]\n",
    "\n",
    "class ChatModelConfig(BaseModel):\n",
    "    model_name: str\n",
    "    \"\"\"모델 제공자가 정한 모델명\"\"\"\n",
    "    temperature: float | None = 0.7\n",
    "    context_length: int\n",
    "    \"\"\"컨텍스트 길이. 모델 제공자가 정한 값과 일치해야 합니다. 조절 불가능한 값입니다.\"\"\"\n",
    "    max_tokens: int\n",
    "    (\n",
    "        \"\"\"최대 생성 토큰 수. 모델 제공자가 정한 값보다 작거나 같아야 합니다. \"\"\"\n",
    "        \"\"\"(context_length - max_tokens)가 최대 입력 토큰 수 이므로, 너무 크게 잡아선 안 됩니다.\"\"\"\n",
    "    )\n",
    "    vision_capability: bool\n",
    "    \"\"\"이미지 입력을 지원하는지 여부. 모델 제공자가 공개한 값과 일치해야 합니다.\"\"\"\n",
    "    knowledge_cutoff: str\n",
    "    \"\"\"모델의 지식 컷오프 날짜. 모델 제공자가 공개한 값과 일치해야 합니다.\"\"\"\n",
    "    default_kwargs: dict[str, Any] = Field(default_factory=dict)\n",
    "    \"\"\"모델 초기화 시 전달할 기본 키워드 인자들.\"\"\"\n",
    "    default_metadata: dict[str, Any] = Field(default_factory=dict)\n",
    "    \"\"\"모델 초기화 시 전달할 기본 langchain 메타데이터.\"\"\"\n",
    "\n",
    "\n",
    "CHAT_MODEL_CONFIG_MAP = {\n",
    "    OpenAIChatModelNamesEnum.GPT4O_MINI: ChatModelConfig(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        context_length=128000,\n",
    "        max_tokens=16384,\n",
    "        vision_capability=True,\n",
    "        knowledge_cutoff=\"October 2023\",\n",
    "    )\n",
    "}\n",
    "\n",
    "def get_max_input_tokens(model_name: ChatModelNamesEnum):\n",
    "    \"\"\"\n",
    "    주어진 모델의 최대 입력 토큰 수를 반환합니다.\n",
    "    \"\"\"\n",
    "    config = CHAT_MODEL_CONFIG_MAP.get(model_name)\n",
    "    if config is None:\n",
    "        raise ValueError(f\"알 수 없는 모델명: {model_name}\")\n",
    "\n",
    "    # 최대 입력 토큰 수는 컨텍스트 길이에서 최대 생성 토큰 수를 뺀 값입니다.\n",
    "    max_input_tokens = config.context_length - config.max_tokens\n",
    "\n",
    "    return max_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111616\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trim_messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 시스템 프롬프트, tool을 고려해서 10% 여유를 둠\u001b[39;00m\n\u001b[32m      7\u001b[39m max_input_tokens = \u001b[38;5;28mint\u001b[39m(max_input_tokens * \u001b[32m0.9\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m message_trimmer = \u001b[43mtrim_messages\u001b[49m(\n\u001b[32m      9\u001b[39m     max_tokens=max_input_tokens,\n\u001b[32m     10\u001b[39m     token_counter=approximate_token_counter,\n\u001b[32m     11\u001b[39m     start_on=HumanMessage,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'trim_messages' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, trim_messages\n",
    "\n",
    "llm_name = OpenAIChatModelNamesEnum.GPT4O_MINI\n",
    "\n",
    "max_input_tokens = get_max_input_tokens(llm_name)\n",
    "\n",
    "print(max_input_tokens)\n",
    "# 시스템 프롬프트, tool을 고려해서 10% 여유를 둠\n",
    "max_input_tokens = int(max_input_tokens * 0.9)\n",
    "message_trimmer = trim_messages(\n",
    "    max_tokens=max_input_tokens,\n",
    "    token_counter=approximate_token_counter,\n",
    "    start_on=HumanMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model_hook = (\n",
    "        (lambda x: x[\"messages\"])\n",
    "        | message_trimmer\n",
    "        | convert_to_fn\n",
    "        | (lambda x: {\"llm_input_messages\": x})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_model_hook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m custom_pre_model_hook = build_pre_model_hook(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     original_hook=\u001b[43mpre_model_hook\u001b[49m,\n\u001b[32m      3\u001b[39m     force_tool_name=\u001b[33m\"\u001b[39m\u001b[33mweb_search\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m web_search_option == \u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      4\u001b[39m     query=query,\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'pre_model_hook' is not defined"
     ]
    }
   ],
   "source": [
    "custom_pre_model_hook = build_pre_model_hook(\n",
    "    original_hook=pre_model_hook,\n",
    "    force_tool_name=\"web_search\" if web_search_option == \"Y\" else None,\n",
    "    query=query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.\"),\n",
    "    HumanMessage(\"최근에 발표된 LangChain 관련 주요 업데이트를 알려줘.\"),\n",
    "    AIMessage(\"LangChain에서는 최근에 'LangGraph' 기능이 도입되었으며, 에이전트 워크플로우를 그래프 형태로 구성할 수 있게 되었습니다. (출처: LangChain 공식 블로그)\"),\n",
    "    HumanMessage(\"LangChain의 공동 창업자인 Harrison은 현재 무슨 프로젝트를 주도하고 있어?\"),\n",
    "    AIMessage(\"Harrison Chase는 LangChain의 CEO로, 최근에는 LangGraph 기반의 멀티에이전트 프레임워크 개발을 리드하고 있습니다. GitHub에서 관련 리포지토리를 운영 중입니다.\"),\n",
    "    HumanMessage(\"웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.\"),\n",
    "]\n",
    "\n",
    "\n",
    "trimmed = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy = \"last\",\n",
    "    token_counter = count_tokens_approximately,\n",
    "    start_on=\"human\",\n",
    "    end_on = (\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='LangChain의 공동 창업자인 Harrison은 현재 무슨 프로젝트를 주도하고 있어?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Harrison Chase는 LangChain의 CEO로, 최근에는 LangGraph 기반의 멀티에이전트 프레임워크 개발을 리드하고 있습니다. GitHub에서 관련 리포지토리를 운영 중입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=5,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len, #각 메시지를 1토큰으로 취급\n",
    "    start_on = \"human\",\n",
    "    end_on = (\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=1,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len, #각 메시지를 1토큰으로 취급\n",
    "    start_on = \"human\",\n",
    "    end_on = (\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_messages = [\n",
    "    AIMessage(\"LangChain에서는 최근에 'LangGraph' 기능이 도입되었으며, 에이전트 워크플로우를 그래프 형태로 구성할 수 있게 되었습니다. (출처: LangChain 공식 블로그)\"),\n",
    "    HumanMessage(\"LangChain의 공동 창업자인 Harrison은 현재 무슨 프로젝트를 주도하고 있어?\"),\n",
    "    AIMessage(\"Harrison Chase는 LangChain의 CEO로, 최근에는 LangGraph 기반의 멀티에이전트 프레임워크 개발을 리드하고 있습니다. GitHub에서 관련 리포지토리를 운영 중입니다.\"),\n",
    "    HumanMessage(\"웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.\"),\n",
    "]\n",
    "\n",
    "\n",
    "trimmed = trim_messages(\n",
    "    sample_messages,\n",
    "    max_tokens=2,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len, #각 메시지를 1토큰으로 취급\n",
    "    start_on = \"human\",\n",
    "    end_on = (\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "trimmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=2,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len, #각 메시지를 1토큰으로 취급\n",
    "    start_on = \"human\",\n",
    "    end_on = (\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "trimmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='최근에 발표된 LangChain 관련 주요 업데이트를 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"first\",\n",
    "    token_counter=count_tokens_approximately,\n",
    ")\n",
    "\n",
    "trimmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Harrison Chase는 LangChain의 CEO로, 최근에는 LangGraph 기반의 멀티에이전트 프레임워크 개발을 리드하고 있습니다. GitHub에서 관련 리포지토리를 운영 중입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(messages, max_tokens=2, include_system=False, token_counter=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(messages, max_tokens=2, include_system=True, token_counter=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='LangChain의 공동 창업자인 Harrison은 현재 무슨 프로젝트를 주도하고 있어?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Harrison Chase는 LangChain의 CEO로, 최근에는 LangGraph 기반의 멀티에이전트 프레임워크 개발을 리드하고 있습니다. GitHub에서 관련 리포지토리를 운영 중입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=4,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len, #각 메시지를 1토큰으로 취급\n",
    "    start_on = \"human\",\n",
    "    end_on = (\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='너는 웹 검색 도구를 활용해 정보를 조사하고 요약해주는 인턴 AI야. 답변할 때는 출처나 맥락을 명확히 전달하고, 필요한 경우 검색 도구를 사용해서 가장 최신 정보를 찾아야 해.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='최근에 발표된 LangChain 관련 주요 업데이트를 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=100,\n",
    "    strategy=\"first\",\n",
    "    token_counter=ChatOpenAI(model='gpt-4o-mini')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='웹 기반 에이전트 개발을 위한 대표적인 오픈소스 예시 알려줘.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "import tiktoken\n",
    "\n",
    "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n",
    "    enc = tiktoken.get_encoding('o200k_base')\n",
    "    return sum(len(enc.encode(msg.content)) +3 for msg in messages)\n",
    "\n",
    "\n",
    "trim_messages(messages, token_counter=tiktoken_counter, max_tokens=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='저는 웹 검색 도구를 사용할 수 없지만, 최신 정보에 대한 질문에 대해 알고 있는 내용을 바탕으로 답변을 드릴 수 있습니다. 질문이 있으면 말씀해 주세요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 61, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BrHQxZ5dXzQED97niPn6U4Yh4wcsD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f06ad4aa-6c56-4204-99cc-81969afe2546-0', usage_metadata={'input_tokens': 61, 'output_tokens': 41, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    start_on=\"human\",\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | llm\n",
    "response = chain.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "history = InMemoryChatMessageHistory(messages=messages[:-1])\n",
    "\n",
    "def get_history(session_id):\n",
    "    return history if session_id == \"1\" else InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(chain, get_history)\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "    [HumanMessage(\"최근 서울 날씨 알려줘\")],\n",
    "    config= {\"configurable\" : {\"session_id\" : \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='알겠습니다! 필요한 정보를 조사하고 요약하여 드리겠습니다. 질문이나 요청이 있으시면 말씀해 주세요.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 61, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BrHU3andVDODdlu005QoEVb5HDXtu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e44f76e3-119a-4ce0-b908-98257ae54be7-0', usage_metadata={'input_tokens': 61, 'output_tokens': 24, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
