{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'서울입니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    result = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "pipeline(\"한국의 수도는\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주요파라미터, invoke(), stream() 스트리밍 출력\n",
    "\n",
    "    - temperature, max_tokens, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='대한민국의 수도는 서울입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 23, 'total_tokens': 38, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-7b738a8f-41da-4ac6-b71b-4c5eee023c43-0' usage_metadata={'input_tokens': 23, 'output_tokens': 15, 'total_tokens': 38}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 객체 생성\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    model_name = 'gpt-3.5-turbo',\n",
    "    \n",
    ")\n",
    "\n",
    "question = \"대한민국 수도는 어디인가요?\"\n",
    "result = llm.invoke(question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국의 수도는 서울입니다.\n",
      "{'refusal': None}\n",
      "{'token_usage': {'completion_tokens': 15, 'prompt_tokens': 23, 'total_tokens': 38, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n",
      "run-7b738a8f-41da-4ac6-b71b-4c5eee023c43-0\n",
      "{'input_tokens': 23, 'output_tokens': 15, 'total_tokens': 38}\n"
     ]
    }
   ],
   "source": [
    "print(result.content)\n",
    "print(result.additional_kwargs)\n",
    "print(result.response_metadata)\n",
    "print(result.id)\n",
    "print(result.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcustom_get_token_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrate_limiter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseRateLimiter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisable_streaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tool_calling'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mroot_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mroot_async_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSecretStr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbase_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0morganization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mopenai_proxy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_logprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtiktoken_model_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdefault_headers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdefault_query\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhttp_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhttp_async_client\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mextra_body\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minclude_response_headers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisabled_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstream_usage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "OpenAI chat model integration.\n",
      "\n",
      ".. dropdown:: Setup\n",
      "    :open:\n",
      "\n",
      "    Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\n",
      "\n",
      "    .. code-block:: bash\n",
      "\n",
      "        pip install -U langchain-openai\n",
      "        export OPENAI_API_KEY=\"your-api-key\"\n",
      "\n",
      ".. dropdown:: Key init args — completion params\n",
      "\n",
      "    model: str\n",
      "        Name of OpenAI model to use.\n",
      "    temperature: float\n",
      "        Sampling temperature.\n",
      "    max_tokens: Optional[int]\n",
      "        Max number of tokens to generate.\n",
      "    logprobs: Optional[bool]\n",
      "        Whether to return logprobs.\n",
      "    stream_options: Dict\n",
      "        Configure streaming outputs, like whether to return token usage when\n",
      "        streaming (``{\"include_usage\": True}``).\n",
      "\n",
      "    See full list of supported init args and their descriptions in the params section.\n",
      "\n",
      ".. dropdown:: Key init args — client params\n",
      "\n",
      "    timeout: Union[float, Tuple[float, float], Any, None]\n",
      "        Timeout for requests.\n",
      "    max_retries: int\n",
      "        Max number of retries.\n",
      "    api_key: Optional[str]\n",
      "        OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\n",
      "    base_url: Optional[str]\n",
      "        Base URL for API requests. Only specify if using a proxy or service\n",
      "        emulator.\n",
      "    organization: Optional[str]\n",
      "        OpenAI organization ID. If not passed in will be read from env\n",
      "        var OPENAI_ORG_ID.\n",
      "\n",
      "    See full list of supported init args and their descriptions in the params section.\n",
      "\n",
      ".. dropdown:: Instantiate\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from langchain_openai import ChatOpenAI\n",
      "\n",
      "        llm = ChatOpenAI(\n",
      "            model=\"gpt-4o\",\n",
      "            temperature=0,\n",
      "            max_tokens=None,\n",
      "            timeout=None,\n",
      "            max_retries=2,\n",
      "            # api_key=\"...\",\n",
      "            # base_url=\"...\",\n",
      "            # organization=\"...\",\n",
      "            # other params...\n",
      "        )\n",
      "\n",
      "    **NOTE**: Any param which is not explicitly supported will be passed directly to the\n",
      "    ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\n",
      "    invoked. For example:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from langchain_openai import ChatOpenAI\n",
      "        import openai\n",
      "\n",
      "        ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n",
      "\n",
      "        # results in underlying API call of:\n",
      "\n",
      "        openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n",
      "\n",
      "        # which is also equivalent to:\n",
      "\n",
      "        ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n",
      "\n",
      ".. dropdown:: Invoke\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        messages = [\n",
      "            (\n",
      "                \"system\",\n",
      "                \"You are a helpful translator. Translate the user sentence to French.\",\n",
      "            ),\n",
      "            (\"human\", \"I love programming.\"),\n",
      "        ]\n",
      "        llm.invoke(messages)\n",
      "\n",
      "    .. code-block:: pycon\n",
      "\n",
      "        AIMessage(\n",
      "            content=\"J'adore la programmation.\",\n",
      "            response_metadata={\n",
      "                \"token_usage\": {\n",
      "                    \"completion_tokens\": 5,\n",
      "                    \"prompt_tokens\": 31,\n",
      "                    \"total_tokens\": 36,\n",
      "                },\n",
      "                \"model_name\": \"gpt-4o\",\n",
      "                \"system_fingerprint\": \"fp_43dfabdef1\",\n",
      "                \"finish_reason\": \"stop\",\n",
      "                \"logprobs\": None,\n",
      "            },\n",
      "            id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
      "            usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
      "        )\n",
      "\n",
      ".. dropdown:: Stream\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        for chunk in llm.stream(messages):\n",
      "            print(chunk)\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      "        AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      "        AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      "        AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      "        AIMessageChunk(\n",
      "            content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n",
      "        )\n",
      "        AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n",
      "        AIMessageChunk(\n",
      "            content=\"\",\n",
      "            response_metadata={\"finish_reason\": \"stop\"},\n",
      "            id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n",
      "        )\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        stream = llm.stream(messages)\n",
      "        full = next(stream)\n",
      "        for chunk in stream:\n",
      "            full += chunk\n",
      "        full\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        AIMessageChunk(\n",
      "            content=\"J'adore la programmation.\",\n",
      "            response_metadata={\"finish_reason\": \"stop\"},\n",
      "            id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n",
      "        )\n",
      "\n",
      ".. dropdown:: Async\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        await llm.ainvoke(messages)\n",
      "\n",
      "        # stream:\n",
      "        # async for chunk in (await llm.astream(messages))\n",
      "\n",
      "        # batch:\n",
      "        # await llm.abatch([messages])\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        AIMessage(\n",
      "            content=\"J'adore la programmation.\",\n",
      "            response_metadata={\n",
      "                \"token_usage\": {\n",
      "                    \"completion_tokens\": 5,\n",
      "                    \"prompt_tokens\": 31,\n",
      "                    \"total_tokens\": 36,\n",
      "                },\n",
      "                \"model_name\": \"gpt-4o\",\n",
      "                \"system_fingerprint\": \"fp_43dfabdef1\",\n",
      "                \"finish_reason\": \"stop\",\n",
      "                \"logprobs\": None,\n",
      "            },\n",
      "            id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n",
      "            usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n",
      "        )\n",
      "\n",
      ".. dropdown:: Tool calling\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "        class GetWeather(BaseModel):\n",
      "            '''Get the current weather in a given location'''\n",
      "\n",
      "            location: str = Field(\n",
      "                ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      "            )\n",
      "\n",
      "\n",
      "        class GetPopulation(BaseModel):\n",
      "            '''Get the current population in a given location'''\n",
      "\n",
      "            location: str = Field(\n",
      "                ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      "            )\n",
      "\n",
      "\n",
      "        llm_with_tools = llm.bind_tools(\n",
      "            [GetWeather, GetPopulation]\n",
      "            # strict = True  # enforce tool args schema is respected\n",
      "        )\n",
      "        ai_msg = llm_with_tools.invoke(\n",
      "            \"Which city is hotter today and which is bigger: LA or NY?\"\n",
      "        )\n",
      "        ai_msg.tool_calls\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        [\n",
      "            {\n",
      "                \"name\": \"GetWeather\",\n",
      "                \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      "                \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"GetWeather\",\n",
      "                \"args\": {\"location\": \"New York, NY\"},\n",
      "                \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"GetPopulation\",\n",
      "                \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      "                \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"GetPopulation\",\n",
      "                \"args\": {\"location\": \"New York, NY\"},\n",
      "                \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n",
      "            },\n",
      "        ]\n",
      "\n",
      "    Note that ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\n",
      "    that defaults to ``True``. This parameter can be set to ``False`` to\n",
      "    disable parallel tool calls:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        ai_msg = llm_with_tools.invoke(\n",
      "            \"What is the weather in LA and NY?\", parallel_tool_calls=False\n",
      "        )\n",
      "        ai_msg.tool_calls\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        [\n",
      "            {\n",
      "                \"name\": \"GetWeather\",\n",
      "                \"args\": {\"location\": \"Los Angeles, CA\"},\n",
      "                \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n",
      "            }\n",
      "        ]\n",
      "\n",
      "    Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\n",
      "    using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\n",
      "    setting ``model_kwargs``.\n",
      "\n",
      "    See ``ChatOpenAI.bind_tools()`` method for more.\n",
      "\n",
      ".. dropdown:: Structured output\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from typing import Optional\n",
      "\n",
      "        from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "        class Joke(BaseModel):\n",
      "            '''Joke to tell user.'''\n",
      "\n",
      "            setup: str = Field(description=\"The setup of the joke\")\n",
      "            punchline: str = Field(description=\"The punchline to the joke\")\n",
      "            rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
      "\n",
      "\n",
      "        structured_llm = llm.with_structured_output(Joke)\n",
      "        structured_llm.invoke(\"Tell me a joke about cats\")\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        Joke(\n",
      "            setup=\"Why was the cat sitting on the computer?\",\n",
      "            punchline=\"To keep an eye on the mouse!\",\n",
      "            rating=None,\n",
      "        )\n",
      "\n",
      "    See ``ChatOpenAI.with_structured_output()`` for more.\n",
      "\n",
      ".. dropdown:: JSON mode\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
      "        ai_msg = json_llm.invoke(\n",
      "            \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n",
      "        )\n",
      "        ai_msg.content\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        '\\n{\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\n}'\n",
      "\n",
      ".. dropdown:: Image input\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        import base64\n",
      "        import httpx\n",
      "        from langchain_core.messages import HumanMessage\n",
      "\n",
      "        image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
      "        image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
      "        message = HumanMessage(\n",
      "            content=[\n",
      "                {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
      "                {\n",
      "                    \"type\": \"image_url\",\n",
      "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
      "                },\n",
      "            ]\n",
      "        )\n",
      "        ai_msg = llm.invoke([message])\n",
      "        ai_msg.content\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n",
      "\n",
      ".. dropdown:: Token usage\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        ai_msg = llm.invoke(messages)\n",
      "        ai_msg.usage_metadata\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
      "\n",
      "    When streaming, set the ``stream_usage`` kwarg:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        stream = llm.stream(messages, stream_usage=True)\n",
      "        full = next(stream)\n",
      "        for chunk in stream:\n",
      "            full += chunk\n",
      "        full.usage_metadata\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n",
      "\n",
      "    Alternatively, setting ``stream_usage`` when instantiating the model can be\n",
      "    useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\n",
      "    methods like ``.with_structured_output``, which generate chains under the\n",
      "    hood.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\n",
      "        structured_llm = llm.with_structured_output(...)\n",
      "\n",
      ".. dropdown:: Logprobs\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        logprobs_llm = llm.bind(logprobs=True)\n",
      "        ai_msg = logprobs_llm.invoke(messages)\n",
      "        ai_msg.response_metadata[\"logprobs\"]\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        {\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"token\": \"J\",\n",
      "                    \"bytes\": [74],\n",
      "                    \"logprob\": -4.9617593e-06,\n",
      "                    \"top_logprobs\": [],\n",
      "                },\n",
      "                {\n",
      "                    \"token\": \"'adore\",\n",
      "                    \"bytes\": [39, 97, 100, 111, 114, 101],\n",
      "                    \"logprob\": -0.25202933,\n",
      "                    \"top_logprobs\": [],\n",
      "                },\n",
      "                {\n",
      "                    \"token\": \" la\",\n",
      "                    \"bytes\": [32, 108, 97],\n",
      "                    \"logprob\": -0.20141791,\n",
      "                    \"top_logprobs\": [],\n",
      "                },\n",
      "                {\n",
      "                    \"token\": \" programmation\",\n",
      "                    \"bytes\": [\n",
      "                        32,\n",
      "                        112,\n",
      "                        114,\n",
      "                        111,\n",
      "                        103,\n",
      "                        114,\n",
      "                        97,\n",
      "                        109,\n",
      "                        109,\n",
      "                        97,\n",
      "                        116,\n",
      "                        105,\n",
      "                        111,\n",
      "                        110,\n",
      "                    ],\n",
      "                    \"logprob\": -1.9361265e-07,\n",
      "                    \"top_logprobs\": [],\n",
      "                },\n",
      "                {\n",
      "                    \"token\": \".\",\n",
      "                    \"bytes\": [46],\n",
      "                    \"logprob\": -1.2233183e-05,\n",
      "                    \"top_logprobs\": [],\n",
      "                },\n",
      "            ]\n",
      "        }\n",
      "\n",
      ".. dropdown:: Response metadata\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        ai_msg = llm.invoke(messages)\n",
      "        ai_msg.response_metadata\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        {\n",
      "            \"token_usage\": {\n",
      "                \"completion_tokens\": 5,\n",
      "                \"prompt_tokens\": 28,\n",
      "                \"total_tokens\": 33,\n",
      "            },\n",
      "            \"model_name\": \"gpt-4o\",\n",
      "            \"system_fingerprint\": \"fp_319be4768e\",\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"logprobs\": None,\n",
      "        }\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/rag/venv/lib/python3.9/site-packages/langchain_openai/chat_models/base.py\n",
      "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "ChatOpenAI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 답변의 형식(AI Message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"캐나다의 수도는 어디인가요\"\n",
    "response = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='캐나다의 수도는 오타와(Ottawa)입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 22, 'total_tokens': 42, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-83a15567-2d2b-41f5-932b-5021e174c6c3-0', usage_metadata={'input_tokens': 22, 'output_tokens': 20, 'total_tokens': 42})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'캐나다의 수도는 오타와(Ottawa)입니다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐나다의 수도는 오타와(Ottawa)입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 20,\n",
       "  'prompt_tokens': 22,\n",
       "  'total_tokens': 42,\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 20,\n",
       " 'prompt_tokens': 22,\n",
       " 'total_tokens': 42,\n",
       " 'completion_tokens_details': {'reasoning_tokens': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata['token_usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata['token_usage']['total_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogProb 활성화\n",
    "\n",
    "    - 주어진 텍스트에 대한 모델의 토큰 확률의 로그 값 의미\n",
    "    토큰(문장을 구성하는 개별 단어나 문자 등의 요소), 확률은 그 모델이 그 토큰을 예측할 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_logprob = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    max_tokens = 2028,\n",
    "    model_name= \"gpt-3.5-turbo\"\n",
    ").bind(logprobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"캐나다의 수도는 어디인가요\"\n",
    "response = llm_with_logprob.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='캐나다의 수도는 오타와(Ottawa)입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 22, 'total_tokens': 42, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [{'token': '\\\\xec', 'bytes': [236], 'logprob': -0.0025401094, 'top_logprobs': []}, {'token': '\\\\xba', 'bytes': [186], 'logprob': -1.378283e-05, 'top_logprobs': []}, {'token': '\\\\x90', 'bytes': [144], 'logprob': -5.5122365e-07, 'top_logprobs': []}, {'token': '나', 'bytes': [235, 130, 152], 'logprob': 0.0, 'top_logprobs': []}, {'token': '다', 'bytes': [235, 139, 164], 'logprob': -1.9361265e-07, 'top_logprobs': []}, {'token': '의', 'bytes': [236, 157, 152], 'logprob': -3.1281633e-07, 'top_logprobs': []}, {'token': ' 수', 'bytes': [32, 236, 136, 152], 'logprob': -9.0883464e-07, 'top_logprobs': []}, {'token': '도', 'bytes': [235, 143, 132], 'logprob': -4.3202e-07, 'top_logprobs': []}, {'token': '는', 'bytes': [235, 138, 148], 'logprob': -1.9361265e-07, 'top_logprobs': []}, {'token': ' \\\\xec\\\\x98', 'bytes': [32, 236, 152], 'logprob': -9.372295e-06, 'top_logprobs': []}, {'token': '\\\\xa4\\\\xed', 'bytes': [164, 237], 'logprob': 0.0, 'top_logprobs': []}, {'token': '\\\\x83', 'bytes': [131], 'logprob': 0.0, 'top_logprobs': []}, {'token': '\\\\x80', 'bytes': [128], 'logprob': -8.299462e-06, 'top_logprobs': []}, {'token': '와', 'bytes': [236, 153, 128], 'logprob': -2.6537622e-05, 'top_logprobs': []}, {'token': '(O', 'bytes': [40, 79], 'logprob': -0.7684058, 'top_logprobs': []}, {'token': 'tt', 'bytes': [116, 116], 'logprob': -1.0280384e-06, 'top_logprobs': []}, {'token': 'awa', 'bytes': [97, 119, 97], 'logprob': -9.133887e-06, 'top_logprobs': []}, {'token': ')', 'bytes': [41], 'logprob': -1.9361265e-07, 'top_logprobs': []}, {'token': '입니다', 'bytes': [236, 158, 133, 235, 139, 136, 235, 139, 164], 'logprob': -0.054059085, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -3.0545007e-06, 'top_logprobs': []}], 'refusal': None}}, id='run-4608022c-ac06-4777-a97d-623f32a73cbd-0', usage_metadata={'input_tokens': 22, 'output_tokens': 20, 'total_tokens': 42})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 20,\n",
       "  'prompt_tokens': 22,\n",
       "  'total_tokens': 42,\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': {'content': [{'token': '\\\\xec',\n",
       "    'bytes': [236],\n",
       "    'logprob': -0.0025401094,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '\\\\xba',\n",
       "    'bytes': [186],\n",
       "    'logprob': -1.378283e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '\\\\x90',\n",
       "    'bytes': [144],\n",
       "    'logprob': -5.5122365e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '나',\n",
       "    'bytes': [235, 130, 152],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '다',\n",
       "    'bytes': [235, 139, 164],\n",
       "    'logprob': -1.9361265e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '의',\n",
       "    'bytes': [236, 157, 152],\n",
       "    'logprob': -3.1281633e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' 수',\n",
       "    'bytes': [32, 236, 136, 152],\n",
       "    'logprob': -9.0883464e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '도',\n",
       "    'bytes': [235, 143, 132],\n",
       "    'logprob': -4.3202e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '는',\n",
       "    'bytes': [235, 138, 148],\n",
       "    'logprob': -1.9361265e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ' \\\\xec\\\\x98',\n",
       "    'bytes': [32, 236, 152],\n",
       "    'logprob': -9.372295e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '\\\\xa4\\\\xed',\n",
       "    'bytes': [164, 237],\n",
       "    'logprob': 0.0,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '\\\\x83', 'bytes': [131], 'logprob': 0.0, 'top_logprobs': []},\n",
       "   {'token': '\\\\x80',\n",
       "    'bytes': [128],\n",
       "    'logprob': -8.299462e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '와',\n",
       "    'bytes': [236, 153, 128],\n",
       "    'logprob': -2.6537622e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '(O',\n",
       "    'bytes': [40, 79],\n",
       "    'logprob': -0.7684058,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'tt',\n",
       "    'bytes': [116, 116],\n",
       "    'logprob': -1.0280384e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'awa',\n",
       "    'bytes': [97, 119, 97],\n",
       "    'logprob': -9.133887e-06,\n",
       "    'top_logprobs': []},\n",
       "   {'token': ')',\n",
       "    'bytes': [41],\n",
       "    'logprob': -1.9361265e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '입니다',\n",
       "    'bytes': [236, 158, 133, 235, 139, 136, 235, 139, 164],\n",
       "    'logprob': -0.054059085,\n",
       "    'top_logprobs': []},\n",
       "   {'token': '.',\n",
       "    'bytes': [46],\n",
       "    'logprob': -3.0545007e-06,\n",
       "    'top_logprobs': []}],\n",
       "  'refusal': None}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍 출력 \n",
    "    - 질의에 대한 답변을 실시간으로 받을 때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.stream(\"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 경복궁 (서울특별시 종로구 사직로 161)\n",
      "2. 남산타워 (서울특별시 용산구 남산공원길 105)\n",
      "3. 부산 해운대해수욕장 (부산광역시 해운대구 해운대해변로 264)\n",
      "4. 경주 석굴암 (경상북도 경주시 남산로 350)\n",
      "5. 제주 용두암 (제주특별자치도 서귀포시 안덕면 용두암로 227)\n",
      "6. 경기도 수원 화성 (경기도 수원시 팔달구 화성로 1)\n",
      "7. 강원도 남이섬 (강원도 춘천시 남산면 남이섬길 1)\n",
      "8. 전주 한옥마을 (전라북도 전주시 완산구 한지길 99)\n",
      "9. 대구 이월드 (대구광역시 달서구 두류공원로 200)\n",
      "10. 전라남도 여수 해상케이블카 (전라남도 여수시 죽림동 케이블카길 123)"
     ]
    }
   ],
   "source": [
    "# 실시간 출력 \n",
    "\n",
    "for token in answer:\n",
    "    print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 경복궁 (서울특별시 종로구 사직로 161)\n",
      "2. 남산서울타워 (서울특별시 용산구 남산공원길 105)\n",
      "3. 부산 해운대해수욕장 (부산광역시 해운대구 해운대해변로 264)\n",
      "4. 제주도 성산일출봉 (제주특별자치도 서귀포시 성산읍 일출로 284)\n",
      "5. 경주 불국사 (경상북도 경주시 불국로 726)\n",
      "6. 강원도 남이섬 (강원도 춘천시 남산면 남이섬길 1)\n",
      "7. 전주 한옥마을 (전라북도 전주시 완산구 한지길 99)\n",
      "8. 대구 이월드 (대구광역시 달서구 두류공원로 200)\n",
      "9. 경기도 남한산성 (경기도 화성시 남양읍 남한산로 200)\n",
      "10. 인천 중구 송도센트럴파크 (인천광역시 중구 송도동)"
     ]
    }
   ],
   "source": [
    "final_answer = \"\"\n",
    "\n",
    "for token in answer:\n",
    "    print(token.content, end = \"\", flush=True)\n",
    "    final_answer += token.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 경복궁 (서울특별시 종로구 사직로 161)\\n2. 남산서울타워 (서울특별시 용산구 남산공원길 105)\\n3. 부산 해운대해수욕장 (부산광역시 해운대구 해운대해변로 264)\\n4. 제주도 성산일출봉 (제주특별자치도 서귀포시 성산읍 일출로 284)\\n5. 경주 불국사 (경상북도 경주시 불국로 726)\\n6. 강원도 남이섬 (강원도 춘천시 남산면 남이섬길 1)\\n7. 전주 한옥마을 (전라북도 전주시 완산구 한지길 99)\\n8. 대구 이월드 (대구광역시 달서구 두류공원로 200)\\n9. 경기도 남한산성 (경기도 화성시 남양읍 남한산로 200)\\n10. 인천 중구 송도센트럴파크 (인천광역시 중구 송도동)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
