{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt template\n",
    "\n",
    "    - PromptTemplate\n",
    "    : 사용자의 입력변수를 사용해서 완전한 프롬프트 문자열 만드는데 사용되는 템플릿\n",
    "        - template : 템플릿 무자열. 이 문자열 내에서 중괄호 {}는 변수를 나타냄\n",
    "        - input_variables : 중괄호 안에 들어갈 변수의 이름을 리스트로 정의\n",
    "\n",
    "    - input_variables\n",
    "        - PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(country = \"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'노르웨이의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(country =\"노르웨이\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = 'gpt-3.5-turbo',\n",
    "    max_tokens=2048,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain 생성\n",
    "\n",
    "    - LCEL(LangChain Expression Language)\n",
    "\n",
    "    LCEL을 사용해 다양한 구성 요소를 단일 체인으로 결합함\n",
    "    chain = prompt | model | output_parser\n",
    "\n",
    "    '|' unix 파이프 연산자와 유사, 서로 다른 구성 요소들을 연겨하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"{topic}에 대해 쉽게 설명해주세요\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='{topic}에 대해 쉽게 설명해주세요')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1199cc950>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1199dcd70>, root_client=<openai.OpenAI object at 0x118e32ae0>, root_async_client=<openai.AsyncOpenAI object at 0x1199cf1d0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### invoke() 호출\n",
    "    - python 딕셔너리 형태로 입력값 전달 (키:값)\n",
    "    - invoke() 함수 소출 시, 입력값을 전달함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"topic\" : \"인공지능 모델의 학습의 원리\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인공지능 모델의 학습은 데이터를 입력으로 받아들이고, 그 데이터를 분석하여 패턴이나 규칙을 찾아내는 과정입니다. 이러한 과정은 크게 데이터를 입력받아 가중치를 조절하고 결과를 출력하는 학습 단계와, 학습된 모델을 테스트하고 정확도를 평가하는 평가 단계로 나뉩니다.\\n\\n먼저, 학습 단계에서는 모델이 입력 데이터를 받아들이고, 내부의 가중치를 조절하여 원하는 결과를 출력합니다. 이때 모델은 입력 데이터와 정답 데이터를 비교하여 오차를 계산하고, 이 오차를 최소화하기 위해 가중치를 조정합니다. 이러한 과정을 반복하면 모델은 학습 데이터에 대해 더 정확한 결과를 출력할 수 있게 됩니다.\\n\\n평가 단계에서는 학습된 모델을 테스트 데이터에 적용하여 정확도를 평가합니다. 이를 통해 모델의 성능을 평가하고, 필요하다면 추가적인 학습을 통해 성능을 개선할 수 있습니다. 이러한 과정을 통해 인공지능 모델은 데이터를 분석하고 패턴을 찾아내는 능력을 향상시키며, 다양한 분야에서 활용될 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 33, 'total_tokens': 443, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2ddbbc4b-23c8-4510-a30b-e6fa50f0754e-0', usage_metadata={'input_tokens': 33, 'output_tokens': 410, 'total_tokens': 443})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(input)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('인공지능 모델의 학습은 데이터를 입력으로 받아들이고, 그 데이터를 분석하여 패턴이나 규칙을 찾아내는 과정입니다. 이러한 과정은 크게 '\n",
      " '데이터를 입력받아 가중치를 조절하고 결과를 출력하는 학습 단계와, 학습된 모델을 테스트하고 정확도를 평가하는 평가 단계로 나뉩니다.\\n'\n",
      " '\\n'\n",
      " '먼저, 학습 단계에서는 모델이 입력 데이터를 받아들이고, 내부의 가중치를 조절하여 원하는 결과를 출력합니다. 이때 모델은 입력 데이터와 '\n",
      " '정답 데이터를 비교하여 오차를 계산하고, 이 오차를 최소화하기 위해 가중치를 조정합니다. 이러한 과정을 반복하면 모델은 학습 데이터에 '\n",
      " '대해 더 정확한 결과를 출력할 수 있게 됩니다.\\n'\n",
      " '\\n'\n",
      " '평가 단계에서는 학습된 모델을 테스트 데이터에 적용하여 정확도를 평가합니다. 이를 통해 모델의 성능을 평가하고, 필요하다면 추가적인 '\n",
      " '학습을 통해 성능을 개선할 수 있습니다. 이러한 과정을 통해 인공지능 모델은 데이터를 분석하고 패턴을 찾아내는 능력을 향상시키며, 다양한 '\n",
      " '분야에서 활용될 수 있습니다.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인공지능 모델의 학습은 데이터를 입력으로 받아들이고, 그 데이터를 분석하여 패턴이나 규칙을 찾아내는 과정입니다. 이러한 과정은 크게 데이터를 입력받아 가중치를 조절하고 결과를 출력하는 학습 단계와, 학습된 모델을 테스트하고 정확도를 평가하는 평가 단계로 나뉩니다.\\n\\n먼저, 학습 단계에서는 모델이 입력 데이터를 받아들이고, 내부의 가중치를 조절하여 원하는 결과를 출력합니다. 이때 모델은 입력 데이터와 정답 데이터를 비교하여 오차를 계산하고, 이 오차를 최소화하기 위해 가중치를 조정합니다. 이러한 과정을 반복하면 모델은 학습 데이터에 대해 더 정확한 결과를 출력할 수 있게 됩니다.\\n\\n평가 단계에서는 학습된 모델을 테스트 데이터에 적용하여 정확도를 평가합니다. 이를 통해 모델의 성능을 평가하고, 필요하다면 추가적인 학습을 통해 성능을 개선할 수 있습니다. 이러한 과정을 통해 인공지능 모델은 데이터를 분석하고 패턴을 찾아내는 능력을 향상시키며, 다양한 분야에서 활용될 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 410, 'prompt_tokens': 33, 'total_tokens': 443, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2ddbbc4b-23c8-4510-a30b-e6fa50f0754e-0', usage_metadata={'input_tokens': 33, 'output_tokens': 410, 'total_tokens': 443})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"{topic}에 대해 {how} 설명해주세요.\")\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'how'}.  Expected: ['how', 'topic'] Received: ['topic']\\nNote: if you intended {how} to be part of the string and not a variable, please escape it with double curly braces like: '{{how}}'.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/geonheekim/Desktop/rag/langchain_grammar/LCEL.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/geonheekim/Desktop/rag/langchain_grammar/LCEL.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39m'\u001b[39m\u001b[39m인공지능 모델의 학습 원리\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/geonheekim/Desktop/rag/langchain_grammar/LCEL.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3018\u001b[0m context\u001b[39m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 3020\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(step\u001b[39m.\u001b[39;49minvoke, \u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3021\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3022\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, \u001b[39minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/prompts/base.py:192\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags:\n\u001b[1;32m    191\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags\n\u001b[0;32m--> 192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m    193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_prompt_with_error_handling,\n\u001b[1;32m    194\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    195\u001b[0m     config,\n\u001b[1;32m    196\u001b[0m     run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    197\u001b[0m     serialized\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_serialized,\n\u001b[1;32m    198\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1923\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m   1920\u001b[0m     context\u001b[39m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1921\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1922\u001b[0m         Output,\n\u001b[0;32m-> 1923\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1924\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1925\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1926\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1927\u001b[0m             config,\n\u001b[1;32m   1928\u001b[0m             run_manager,\n\u001b[1;32m   1929\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1930\u001b[0m         ),\n\u001b[1;32m   1931\u001b[0m     )\n\u001b[1;32m   1932\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1933\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/prompts/base.py:166\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_format_prompt_with_error_handling\u001b[39m(\u001b[39mself\u001b[39m, inner_input: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m--> 166\u001b[0m     _inner_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_input(inner_input)\n\u001b[1;32m    167\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_inner_input)\n",
      "File \u001b[0;32m~/Desktop/rag/venv/lib/python3.12/site-packages/langchain_core/prompts/base.py:162\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    156\u001b[0m     example_key \u001b[39m=\u001b[39m missing\u001b[39m.\u001b[39mpop()\n\u001b[1;32m    157\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mNote: if you intended \u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m{\u001b[39;00mexample_key\u001b[39m}\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m to be part of the string\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m{{\u001b[39;00m\u001b[39m{\u001b[39;00mexample_key\u001b[39m}\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(msg)\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m inner_input\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Input to PromptTemplate is missing variables {'how'}.  Expected: ['how', 'topic'] Received: ['topic']\\nNote: if you intended {how} to be part of the string and not a variable, please escape it with double curly braces like: '{{how}}'.\""
     ]
    }
   ],
   "source": [
    "input= {\"topic\" : '인공지능 모델의 학습 원리'}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"topic\" : \"인공지능 모델의 학습 원리\" , \"how\" : \"영어로\"}\n",
    "\n",
    "result = chain.invoke(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Artificial intelligence models learn through a process called machine learning, where they are trained on a large dataset to identify patterns and relationships within the data. The model adjusts its parameters based on the feedback it receives during training in order to minimize errors and improve its performance. This process allows the model to make predictions or decisions based on new input data that it has not seen before. Over time, the model becomes more accurate and effective at solving the specific task it was designed for.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 33, 'total_tokens': 127, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c107652e-5aa4-46bb-8bf5-eab875b67a20-0', usage_metadata={'input_tokens': 33, 'output_tokens': 94, 'total_tokens': 127})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Artificial intelligence models learn through a process called machine '\n",
      " 'learning, where they are trained on a large dataset to identify patterns and '\n",
      " 'relationships within the data. The model adjusts its parameters based on the '\n",
      " 'feedback it receives during training in order to minimize errors and improve '\n",
      " 'its performance. This process allows the model to make predictions or '\n",
      " 'decisions based on new input data that it has not seen before. Over time, '\n",
      " 'the model becomes more accurate and effective at solving the specific task '\n",
      " 'it was designed for.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 94,\n",
       "  'prompt_tokens': 33,\n",
       "  'total_tokens': 127,\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  출력파서(Output Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"{topic}에 대해 쉽게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"topic\" : \"인공지능 모델의 학습 원리\"}\n",
    "\n",
    "result = chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능 모델의 학습 원리는 데이터를 입력으로 받아서 패턴을 학습하는 과정입니다. 모델은 많은 양의 데이터를 통해 입력과 출력 간의 관계를 학습하여 예측하거나 분류하는 능력을 갖춥니다. \\n\\n이를 위해 모델은 초기에는 무작위로 설정된 가중치와 편향을 가지고 있습니다. 학습 과정에서 모델은 입력 데이터를 받아 예측을 수행하고, 이 예측 값과 실제 값 간의 오차를 계산합니다. 이 오차를 최소화하기 위해 모델은 손실 함수를 사용하여 가중치와 편향을 조정하고, 다시 데이터를 통해 학습을 반복합니다.\\n\\n이러한 과정을 반복하면 모델은 점차적으로 정확한 예측을 할 수 있도록 학습하게 됩니다. 이렇게 학습된 모델은 새로운 데이터에 대해 정확한 예측을 수행할 수 있게 됩니다.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿을 변경하여 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\\n양식은 [FORMAT]을 참고하여 작성해 주세요.\\n\\n#상황:\\n{{question}}\\n\\n#FORMAT:\\n- 영어 회화:\\n- 한글 해석:\\n')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{{question}}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\n당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\\n양식은 [FORMAT]을 참고하여 작성해 주세요.\\n\\n#상황:\\n{question}\\n\\n#FORMAT:\\n- 영어 회화:\\n- 한글 해석:\\n')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\n당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\\n양식은 [FORMAT]을 참고하여 작성해 주세요.\\n\\n#상황:\\n{question}\\n\\n#FORMAT:\\n- 영어 회화:\\n- 한글 해석:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11a7f9a00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x11a7f95b0>, root_client=<openai.OpenAI object at 0x11a76c0e0>, root_async_client=<openai.AsyncOpenAI object at 0x11a7f97f0>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화: Can I order some food, please?\n",
      "- 한글 해석: 음식 주문할 수 있을까요?\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"question\" : \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화: Hi, I'd like to order a large pepperoni pizza for delivery, please.\n",
      "- 한글 해석: 안녕하세요, 배달로 대형 페퍼로니 피자 주문하고 싶어요.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"question\" : \"미국에서 피자 주문\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#상황:\n",
      "미국에서 피자 주문\n",
      "\n",
      "#FORMAT:\n",
      "- 영어 회화:\n",
      "Customer: Hi, I'd like to order a large pepperoni pizza for delivery, please.\n",
      "Employee: Sure, would you like any drinks or sides with that?\n",
      "Customer: Yes, I'll also take a 2-liter soda and an order of garlic knots.\n",
      "Employee: Great, your total comes to $25. Will that be cash or card?\n",
      "Customer: I'll pay with my card, thank you.\n",
      "\n",
      "- 한글 해석:\n",
      "고객: 안녕하세요, 배달로 대형 페퍼로니 피자 주문하려고 합니다.\n",
      "직원: 네, 그럼 음료나 사이드 디시트 더 추가하실 건가요?\n",
      "고객: 네, 2리터 콜라 하나랑 마늘 노트 주문할게요.\n",
      "직원: 좋아요, 총 금액은 25달러 입니다. 현금으로 결제하실 건가요, 아니면 카드로 결제하시겠어요?\n",
      "고객: 카드로 결제할게요, 감사합니다."
     ]
    }
   ],
   "source": [
    "async for event in chain.astream_events({\"question\" : \"미국에서 피자 주문\"}, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
