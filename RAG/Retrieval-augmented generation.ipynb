{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767e71bf-e1e5-4ec3-9976-034baa3396d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/site-packages (0.0.345)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.351-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/site-packages (0.4.18)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.4.20-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
      "  Downloading langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langchain-core<0.2,>=0.1 (from langchain)\n",
      "  Downloading langchain_core-0.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.70 (from langchain)\n",
      "  Downloading langsmith-0.0.72-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/site-packages (from langchain) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.104.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0.post1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (3.1.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.16.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.15.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/site-packages (from chromadb) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (1.59.3)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.1.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/site-packages (from chromadb) (28.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/site-packages (from chromadb) (4.0.1)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.24.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in /usr/local/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.21.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.21.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.42b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.42b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.42b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.42b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.42b0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.42b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading types_requests-2.31.0.9-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading types_requests-2.31.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading types_requests-2.31.0.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting types-urllib3 (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub)\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "Downloading langchain-0.0.351-py3-none-any.whl (794 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.6.0-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-0.4.20-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.7/507.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Downloading langchain_community-0.0.4-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.72-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=95af4523ea329e5c9e313d06073e53f4c08f1a4c1c954c1f4b2787bb1f05d426\n",
      "  Stored in directory: /Users/geonheekim/Library/Caches/pip/wheels/d4/c8/5b/b5be9c20e5e4503d04a6eac8a3cd5c2393505c29f02bea0960\n",
      "Successfully built bs4\n",
      "Installing collected packages: types-urllib3, types-requests, langchainhub, bs4, openai, langsmith, langchain-core, langchain-community, langchain, chromadb\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.69\n",
      "    Uninstalling langsmith-0.0.69:\n",
      "      Successfully uninstalled langsmith-0.0.69\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.0.9\n",
      "    Uninstalling langchain-core-0.0.9:\n",
      "      Successfully uninstalled langchain-core-0.0.9\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.345\n",
      "    Uninstalling langchain-0.0.345:\n",
      "      Successfully uninstalled langchain-0.0.345\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 0.4.18\n",
      "    Uninstalling chromadb-0.4.18:\n",
      "      Successfully uninstalled chromadb-0.4.18\n",
      "Successfully installed bs4-0.0.1 chromadb-0.4.20 langchain-0.0.351 langchain-community-0.0.4 langchain-core-0.1.1 langchainhub-0.1.14 langsmith-0.0.72 openai-1.6.0 types-requests-2.31.0.6 types-urllib3-1.26.25.14\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U langchain openai chromadb langchainhub bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c21dab7-cfb9-40e6-9948-e9293b029188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea19b0b7-0509-442f-905e-e094371d64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf4833f-1c92-4b17-b15d-5f58c085a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fac600-d3a3-4659-8c33-f2c0f81a5b27",
   "metadata": {},
   "source": [
    "## step 1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a7f1b9-9509-4b6b-8605-a61e6c3d71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0753019f-0e78-4542-afd7-baf92d78286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs= {\n",
    "        \"parse_only\" : bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ef755a-3ce4-4a94-add5-a6790a880823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "141ee8fa-64e6-4632-b40d-b02abb59b50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f361608-53b7-4868-8e01-733528e683a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85253c32-bdf8-4c1f-901b-b693ee5cb9f1",
   "metadata": {},
   "source": [
    "## Step 2. Split\n",
    "\n",
    "    로드된 문서의 길이는 42,000자를 넘습니다. 이는 너무 길어서 많은 모델의 컨텍스트 창에 맞지 않습니다.\n",
    "    그리고 컨텍스트 창에 전체 게시물을 맞출 수 있는 모델의 경우에도 경험적으로 모델은 매우 긴 프롬프트에서 관련 컨텍스트를 찾는 데 어려움을 겪습니다.\n",
    "    따라서 삽입 및 벡터 저장을 위해 문서를 청크로 분할합니다.\n",
    "    이는 런타임 시 블로그 게시물의 가장 관련성이 높은 부분만 검색하는 데 도움이 됩니다.\n",
    "    이 경우 문서를 1000자의 청크로 분할하고 청크 사이에 200자가 겹치도록 하겠습니다.\n",
    "    중복은 진술과 관련된 중요한 맥락에서 진술을 분리할 가능성을 완화하는 데 도움이 됩니다.\n",
    "    여기서는 RecursiveCharacterTextSplitter를 사용합니다. 각 청크가 적절한 크기가 될 때까지 공통 구분 기호(예: 새 줄)를 사용하여 문서를 (재귀적으로) 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6d73440-5c7f-4dbb-8814-e86103cb7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77b71114-1371-4963-881f-505880bcd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c4bab45-f911-4bc4-9667-15b480ac904b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4651887b-c3ef-45d5-82a5-7f336a72246e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a8d6f28-4023-4900-85f1-2324910b332f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[13] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[14] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[15] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[16] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[17] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[18] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 42174})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40792dbb-6ffa-498a-9809-2591fa804eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc200ee-f076-41ed-bbec-d43f9a6acfee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbf4507-7a6b-49ca-9dc1-64bd099885be",
   "metadata": {},
   "source": [
    "## 3. Store\n",
    "    \n",
    "    이제 메모리에 66개의 텍스트 청크가 있으므로 나중에 RAG 앱에서 검색할 수 있도록 이를 저장하고 색인화해야 합니다. \n",
    "    이를 수행하는 가장 일반적인 방법은 분할된 각 문서의 콘텐츠를 포함하고 해당 포함을 벡터 저장소에 업로드하는 것입니다.\n",
    "    그런 다음 분할을 검색하려는 경우 검색 쿼리를 가져와 이를 포함하고 일종의 \"유사성\" 검색을 수행하여 쿼리 포함과 가장 유사한 포함으로 저장된 분할을 식별합니다.\n",
    "    가장 간단한 유사성 측정은 코사인 유사성입니다. 즉, 각 임베딩 쌍(매우 고차원 벡터임) 사이의 각도의 코사인을 측정합니다.\n",
    "    Chroma 벡터 저장소와 OpenAIEmbeddings 모델을 사용하여 단일 명령에 모든 문서 분할을 포함하고 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaa99bda-3168-4c8a-b679-46c6204d67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c25a181-5bf2-4c82-b7b0-5dd692983928",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03da8fa-916a-4b43-b966-f13352e33d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c6761c-d32f-4fbb-8224-93e4a38676c2",
   "metadata": {},
   "source": [
    "## 4. Retrieve\n",
    "    \n",
    "    실제 애플리케이션 로직을 작성합니다.\n",
    "    사용자가 질문하고, 해당 질문과 관련된 문서를 검색하고, 검색된 문서와 초기 질문을 모델에 전달하고, 최종적으로 답변을 반환하는 간단한 애플리케이션을 작성합니다.\n",
    "    LangChain은 문자열 쿼리가 주어지면 관련 문서를 반환할 수 있는 인덱스를 래핑하는 Retriever 인터페이스를 정의합니다. \n",
    "    모든 검색기는 공통 메소드 get_relevant_documents()(및 그 비동기 변형 aget_relevant_documents())를 구현합니다.\n",
    "    가장 일반적인 유형의 Retriever는 벡터 저장소의 유사성 검색 기능을 사용하여 검색을 용이하게 하는 VectorStoreRetriever입니다.\n",
    "    모든 VectorStore는 쉽게 Retriever로 전환될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33f936b6-215e-4de0-96da-568c8a1d59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c43e26dd-ec88-4dde-9a88-73b810254186",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs= retriever.get_relevant_documents(\n",
    "    \"What are the approaches to Task Decompostion?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e65b91c5-93df-4d8e-b111-e1818742c55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5a05c12-4167-40de-a49e-7f1b8b26ae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4c1dd-3acc-444a-b5ff-3829c6ebd317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6f4bbf6-97bf-4f50-aacb-1c68a894fa4e",
   "metadata": {},
   "source": [
    "## Step 5. Generate\n",
    "    \n",
    "    질문을 받고, 관련 문서를 검색하고, 프롬프트를 구성하고, 이를 모델에 전달하고, 출력을 구문 분석하는 체인에 모두 함께 넣습니다.\n",
    "    gpt-3.5-turbo OpenAI 채팅 모델을 사용할 것이지만 LangChain LLM 또는 ChatModel을 대체할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "114c23bf-437a-4c81-94a4-58ca9effad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39fb2e12-b029-4f1d-b4a1-2d3b9216ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6898498-f801-4e41-9d42-ebe5c43d9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull('rlm/rag-prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f438baf0-f794-425a-8a15-d994669c76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filter context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.invoke(\n",
    "    {'context':'filter context', 'question' : 'filler question'}\n",
    ").to_string()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab175b26-da6c-489e-95d9-08c1de74404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d5401f4-bf77-4064-9d48-873b444ebc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {'context' : retriever | format_docs, 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46fdf197-cd91-4625-b2e0-14011556a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and interpretable thinking process. This can be done through prompting techniques like Chain of Thought or Tree of Thoughts, as well as task-specific instructions or human inputs."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream('What is Task Decompostion?'):\n",
    "    print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f826a-0cc0-414a-a805-f7963e9ce919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeeec98-e785-48b2-a7c0-fa23332ec3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc66b3-5d2f-496e-b425-4dd1de1f4d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e153d08-c5d3-4f5e-a717-9dd93108df1e",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cdb77-e892-439f-93ab-a5c7e8968009",
   "metadata": {},
   "source": [
    "### Customizing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3956a343-1c6a-4bcc-87c8-816a0e099b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f985e644-f8eb-455d-bb95-444992f83e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741c38d-7b1c-4885-9277-26f8c6428917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239110c-4229-4e5b-abd3-806d5fd40d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7498929-dd13-44d2-9db1-b3b07fd682b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d23d9e8-1069-4a20-a697-208e23bca1f1",
   "metadata": {},
   "source": [
    "`LCEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3adad2ed-4938-41a5-9a88-3a8dd07b7679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 1585},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 2192},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 17804},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 17414},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 29630},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 19373}],\n",
       " 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f55cf5-7311-496c-ae04-e2062a7fbb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5466aa-1256-40a7-88f6-e8d779a3ef4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4edd5b35-34e5-48a3-9007-916e7dc1ce83",
   "metadata": {},
   "source": [
    "`Adding memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d9fb6-d05f-4eed-a697-f4dd3d7924ab",
   "metadata": {},
   "source": [
    "- 과거 사용자 입력을 기억하는 상태 저장 애플리케이션을 만들고 싶다고 가정한다면, 이를 지원하기 위해 우리가 해야 할 일은 크게 두 가지입니다.\n",
    "- 기록 메시지를 전달할 수 있는 메시지 자리 표시자를 체인에 추가합니다.\n",
    "- 최신 사용자 쿼리를 가져와서 채팅 기록의 맥락에서 검색자에게 전달할 수 있는 독립형 질문으로 재구성하는 체인을 추가합니다.\n",
    "- 다음과 같은 \"질문 요약\" 체인을 구축할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70bf1e38-c0e5-44af-87a8-f50f273fcd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27d4f2bb-85a2-4d84-805d-9a4d51adf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference the chat histor, formulate a stadalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otehrwise return it as is.\"\"\"\n",
    "\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d3c7741-f600-4032-bef5-61681c15b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of \"large\" in the context of a language model?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25fd9ccc-0a11-4f96-8000-3f3a8e7fad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do transformer models function?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"How do transformers work\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac3c916a-8e8e-46f9-859b-94029953b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def condense_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return condense_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3733c75a-7594-4f3c-93fd-3138612483c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. This approach utilizes more computation at test-time and sheds light on the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" This method guides the model to break down the task into manageable steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction \"Write a story outline\" can be given to help the model break down the task into smaller components.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide insights, expertise, and domain knowledge to help break down complex tasks into smaller subtasks.\\n\\nThese approaches aim to simplify complex tasks and enable more effective problem-solving and planning.')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1a4a3-6ce6-41f1-b6b8-f9d40480b4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
